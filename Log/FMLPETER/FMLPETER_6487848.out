-------------------------PETER ARGUMENTS-------------------------
data_path            FMLPETER/data/Movie_and_TV_GT5.pickle
index_dir            FMLPETER/data/0
checkpoint           FMLPETER/output
outf                 generated.txt
emsize               512
nhead                2
nhid                 2048
nlayers              2
dropout              0.2
lr                   1.0
clip                 1.0
epochs               100
batch_size           128
seed                 1111
cuda                 True
log_interval         1000
max_hist_len         20
vocab_size           20000
endure_times         5
rating_reg           0.1
context_reg          1.0
text_reg             1.0
peter_mask           True
use_feature          True
words                15
mode                 Train+Test
model_path           FMLPETER/output/model.pt
prediction_path      FMLPETER/output/generated.txt
-------------------------PETER ARGUMENTS-------------------------
[2022-12-13 05:29:19.22]: Loading data
[2022-12-13 05:31:47.79]: Build Model
-------------------------FMLP ARGUMENTS-------------------------
data_dir                       FMLPETER/data/
output_dir                     FMLPETER/output/
data_name                      Movie_and_TV_GT5
do_eval                        True
load_model                     FMLPRec-Movie_and_TV_GT5-Dec-09-2022_17-02-48
model_name                     FMLPRec
hidden_size                    512
num_hidden_layers              4
num_attention_heads            2
hidden_act                     gelu
attention_probs_dropout_prob   0.5
hidden_dropout_prob            0.5
initializer_range              0.02
max_seq_length                 20
no_filters                     False
lr                             0.001
batch_size                     512
epochs                         100
no_cuda                        False
log_freq                       1
full_sort                      False
patience                       5
seed                           42
weight_decay                   0.0
adam_beta1                     0.9
adam_beta2                     0.99
gpu_id                         0
variance                       5
cuda_condition                 True
log_file                       FMLPETER/output/FMLPRec-Movie_and_TV_GT5-2022-12-13 05:31:50.26.txt
-------------------------FMLP ARGUMENTS-------------------------
Total Parameters: 12232192
original state odict_keys(['item_embeddings.weight', 'position_embeddings.weight', 'LayerNorm.weight', 'LayerNorm.bias', 'item_encoder.layer.0.filterlayer.complex_weight', 'item_encoder.layer.0.filterlayer.LayerNorm.weight', 'item_encoder.layer.0.filterlayer.LayerNorm.bias', 'item_encoder.layer.0.intermediate.dense_1.weight', 'item_encoder.layer.0.intermediate.dense_1.bias', 'item_encoder.layer.0.intermediate.dense_2.weight', 'item_encoder.layer.0.intermediate.dense_2.bias', 'item_encoder.layer.0.intermediate.LayerNorm.weight', 'item_encoder.layer.0.intermediate.LayerNorm.bias', 'item_encoder.layer.1.filterlayer.complex_weight', 'item_encoder.layer.1.filterlayer.LayerNorm.weight', 'item_encoder.layer.1.filterlayer.LayerNorm.bias', 'item_encoder.layer.1.intermediate.dense_1.weight', 'item_encoder.layer.1.intermediate.dense_1.bias', 'item_encoder.layer.1.intermediate.dense_2.weight', 'item_encoder.layer.1.intermediate.dense_2.bias', 'item_encoder.layer.1.intermediate.LayerNorm.weight', 'item_encoder.layer.1.intermediate.LayerNorm.bias', 'item_encoder.layer.2.filterlayer.complex_weight', 'item_encoder.layer.2.filterlayer.LayerNorm.weight', 'item_encoder.layer.2.filterlayer.LayerNorm.bias', 'item_encoder.layer.2.intermediate.dense_1.weight', 'item_encoder.layer.2.intermediate.dense_1.bias', 'item_encoder.layer.2.intermediate.dense_2.weight', 'item_encoder.layer.2.intermediate.dense_2.bias', 'item_encoder.layer.2.intermediate.LayerNorm.weight', 'item_encoder.layer.2.intermediate.LayerNorm.bias', 'item_encoder.layer.3.filterlayer.complex_weight', 'item_encoder.layer.3.filterlayer.LayerNorm.weight', 'item_encoder.layer.3.filterlayer.LayerNorm.bias', 'item_encoder.layer.3.intermediate.dense_1.weight', 'item_encoder.layer.3.intermediate.dense_1.bias', 'item_encoder.layer.3.intermediate.dense_2.weight', 'item_encoder.layer.3.intermediate.dense_2.bias', 'item_encoder.layer.3.intermediate.LayerNorm.weight', 'item_encoder.layer.3.intermediate.LayerNorm.bias'])
new state odict_keys(['item_embeddings.weight', 'position_embeddings.weight', 'LayerNorm.weight', 'LayerNorm.bias', 'item_encoder.layer.0.filterlayer.complex_weight', 'item_encoder.layer.0.filterlayer.LayerNorm.weight', 'item_encoder.layer.0.filterlayer.LayerNorm.bias', 'item_encoder.layer.0.intermediate.dense_1.weight', 'item_encoder.layer.0.intermediate.dense_1.bias', 'item_encoder.layer.0.intermediate.dense_2.weight', 'item_encoder.layer.0.intermediate.dense_2.bias', 'item_encoder.layer.0.intermediate.LayerNorm.weight', 'item_encoder.layer.0.intermediate.LayerNorm.bias', 'item_encoder.layer.1.filterlayer.complex_weight', 'item_encoder.layer.1.filterlayer.LayerNorm.weight', 'item_encoder.layer.1.filterlayer.LayerNorm.bias', 'item_encoder.layer.1.intermediate.dense_1.weight', 'item_encoder.layer.1.intermediate.dense_1.bias', 'item_encoder.layer.1.intermediate.dense_2.weight', 'item_encoder.layer.1.intermediate.dense_2.bias', 'item_encoder.layer.1.intermediate.LayerNorm.weight', 'item_encoder.layer.1.intermediate.LayerNorm.bias', 'item_encoder.layer.2.filterlayer.complex_weight', 'item_encoder.layer.2.filterlayer.LayerNorm.weight', 'item_encoder.layer.2.filterlayer.LayerNorm.bias', 'item_encoder.layer.2.intermediate.dense_1.weight', 'item_encoder.layer.2.intermediate.dense_1.bias', 'item_encoder.layer.2.intermediate.dense_2.weight', 'item_encoder.layer.2.intermediate.dense_2.bias', 'item_encoder.layer.2.intermediate.LayerNorm.weight', 'item_encoder.layer.2.intermediate.LayerNorm.bias', 'item_encoder.layer.3.filterlayer.complex_weight', 'item_encoder.layer.3.filterlayer.LayerNorm.weight', 'item_encoder.layer.3.filterlayer.LayerNorm.bias', 'item_encoder.layer.3.intermediate.dense_1.weight', 'item_encoder.layer.3.intermediate.dense_1.bias', 'item_encoder.layer.3.intermediate.dense_2.weight', 'item_encoder.layer.3.intermediate.dense_2.bias', 'item_encoder.layer.3.intermediate.LayerNorm.weight', 'item_encoder.layer.3.intermediate.LayerNorm.bias'])
Load model from FMLPETER/output/FMLPRec-Movie_and_TV_GT5-Dec-09-2022_17-02-48.pt for test!
[2022-12-13 05:32:02.64]: epoch 1
[2022-12-13 05:32:57.44]: context ppl 957.8475 | text ppl 397.0892 | rating loss 1.8959 |  1000/ 2583 batches
[2022-12-13 05:33:51.90]: context ppl 648.9254 | text ppl 150.7506 | rating loss 1.3328 |  2000/ 2583 batches
[2022-12-13 05:34:23.72]: context ppl 620.8618 | text ppl 125.0374 | rating loss 1.3244 |  2583/ 2583 batches
[2022-12-13 05:34:29.08]: context ppl 612.5234 | text ppl 106.5625 | rating loss 1.2904 | valid loss 5.9591 on validation
[2022-12-13 05:34:29.61]: epoch 2
[2022-12-13 05:35:24.63]: context ppl 608.2138 | text ppl 106.8502 | rating loss 1.2673 |  1000/ 2583 batches
[2022-12-13 05:36:19.69]: context ppl 596.1824 | text ppl 96.6132 | rating loss 1.2656 |  2000/ 2583 batches
[2022-12-13 05:36:51.75]: context ppl 590.8307 | text ppl 89.7918 | rating loss 1.2536 |  2583/ 2583 batches
[2022-12-13 05:36:57.15]: context ppl 583.7100 | text ppl 79.8291 | rating loss 1.1917 | valid loss 5.5716 on validation
[2022-12-13 05:36:57.66]: epoch 3
[2022-12-13 05:37:53.27]: context ppl 581.2599 | text ppl 81.6809 | rating loss 1.2490 |  1000/ 2583 batches
[2022-12-13 05:38:48.71]: context ppl 579.6286 | text ppl 78.7190 | rating loss 1.2436 |  2000/ 2583 batches
[2022-12-13 05:39:20.90]: context ppl 574.7876 | text ppl 75.8846 | rating loss 1.2299 |  2583/ 2583 batches
[2022-12-13 05:39:26.34]: context ppl 565.3928 | text ppl 67.5981 | rating loss 1.1720 | valid loss 5.3856 on validation
[2022-12-13 05:39:26.85]: epoch 4
[2022-12-13 05:40:22.79]: context ppl 569.8529 | text ppl 70.4007 | rating loss 1.2316 |  1000/ 2583 batches
[2022-12-13 05:41:18.52]: context ppl 566.3139 | text ppl 69.1036 | rating loss 1.2297 |  2000/ 2583 batches
[2022-12-13 05:41:50.94]: context ppl 567.5761 | text ppl 68.5545 | rating loss 1.2131 |  2583/ 2583 batches
[2022-12-13 05:41:56.40]: context ppl 561.2101 | text ppl 63.5384 | rating loss 1.1668 | valid loss 5.3185 on validation
[2022-12-13 05:41:56.93]: epoch 5
[2022-12-13 05:42:52.94]: context ppl 560.9922 | text ppl 63.9557 | rating loss 1.2103 |  1000/ 2583 batches
[2022-12-13 05:43:48.75]: context ppl 559.7004 | text ppl 63.3291 | rating loss 1.2150 |  2000/ 2583 batches
[2022-12-13 05:44:21.23]: context ppl 558.6675 | text ppl 62.7971 | rating loss 1.2094 |  2583/ 2583 batches
[2022-12-13 05:44:26.65]: context ppl 556.3795 | text ppl 59.8919 | rating loss 1.1496 | valid loss 5.2421 on validation
[2022-12-13 05:44:27.17]: epoch 6
[2022-12-13 05:45:23.03]: context ppl 553.3359 | text ppl 58.8973 | rating loss 1.2085 |  1000/ 2583 batches
[2022-12-13 05:46:18.92]: context ppl 554.0285 | text ppl 59.5490 | rating loss 1.2019 |  2000/ 2583 batches
[2022-12-13 05:46:51.51]: context ppl 551.9956 | text ppl 58.8318 | rating loss 1.1853 |  2583/ 2583 batches
[2022-12-13 05:46:56.95]: context ppl 549.3481 | text ppl 56.1600 | rating loss 1.1448 | valid loss 5.1730 on validation
[2022-12-13 05:46:57.48]: epoch 7
[2022-12-13 05:47:53.42]: context ppl 548.2696 | text ppl 55.6876 | rating loss 1.1950 |  1000/ 2583 batches
[2022-12-13 05:48:49.24]: context ppl 548.6539 | text ppl 55.8840 | rating loss 1.1927 |  2000/ 2583 batches
[2022-12-13 05:49:21.77]: context ppl 546.0708 | text ppl 55.9821 | rating loss 1.1870 |  2583/ 2583 batches
[2022-12-13 05:49:27.22]: context ppl 545.8093 | text ppl 55.0643 | rating loss 1.1682 | valid loss 5.1767 on validation
[2022-12-13 05:49:27.22]: Endured 1 time(s)
[2022-12-13 05:49:27.22]: Learning rate set to 0.25000000
[2022-12-13 05:49:27.22]: epoch 8
[2022-12-13 05:50:22.96]: context ppl 535.3246 | text ppl 48.5990 | rating loss 1.1725 |  1000/ 2583 batches
[2022-12-13 05:51:18.56]: context ppl 533.6998 | text ppl 47.8881 | rating loss 1.1654 |  2000/ 2583 batches
[2022-12-13 05:51:51.04]: context ppl 534.0364 | text ppl 47.6454 | rating loss 1.1572 |  2583/ 2583 batches
[2022-12-13 05:51:56.48]: context ppl 538.6249 | text ppl 50.4507 | rating loss 1.1290 | valid loss 5.0499 on validation
[2022-12-13 05:51:57.03]: epoch 9
[2022-12-13 05:52:52.70]: context ppl 531.7034 | text ppl 46.1986 | rating loss 1.1618 |  1000/ 2583 batches
[2022-12-13 05:53:48.79]: context ppl 531.0688 | text ppl 46.3579 | rating loss 1.1646 |  2000/ 2583 batches
[2022-12-13 05:54:21.52]: context ppl 532.7930 | text ppl 46.2282 | rating loss 1.1621 |  2583/ 2583 batches
[2022-12-13 05:54:26.98]: context ppl 536.1559 | text ppl 49.7324 | rating loss 1.1221 | valid loss 5.0288 on validation
[2022-12-13 05:54:27.49]: epoch 10
[2022-12-13 05:55:24.16]: context ppl 528.5759 | text ppl 44.9590 | rating loss 1.1660 |  1000/ 2583 batches
[2022-12-13 05:56:20.59]: context ppl 530.0120 | text ppl 45.3001 | rating loss 1.1514 |  2000/ 2583 batches
[2022-12-13 05:56:53.38]: context ppl 531.8090 | text ppl 45.6612 | rating loss 1.1647 |  2583/ 2583 batches
[2022-12-13 05:56:58.83]: context ppl 534.4873 | text ppl 49.1472 | rating loss 1.1189 | valid loss 5.0137 on validation
[2022-12-13 05:56:59.33]: epoch 11
[2022-12-13 05:57:55.72]: context ppl 528.8530 | text ppl 44.2160 | rating loss 1.1546 |  1000/ 2583 batches
[2022-12-13 05:58:51.93]: context ppl 527.9121 | text ppl 44.3481 | rating loss 1.1588 |  2000/ 2583 batches
[2022-12-13 05:59:24.75]: context ppl 527.0779 | text ppl 44.6627 | rating loss 1.1547 |  2583/ 2583 batches
[2022-12-13 05:59:30.18]: context ppl 533.2526 | text ppl 48.6030 | rating loss 1.1182 | valid loss 5.0019 on validation
[2022-12-13 05:59:30.70]: epoch 12
[2022-12-13 06:00:26.90]: context ppl 525.0190 | text ppl 43.4099 | rating loss 1.1531 |  1000/ 2583 batches
[2022-12-13 06:01:23.42]: context ppl 526.7233 | text ppl 43.6528 | rating loss 1.1604 |  2000/ 2583 batches
[2022-12-13 06:01:56.36]: context ppl 527.6931 | text ppl 43.8272 | rating loss 1.1516 |  2583/ 2583 batches
[2022-12-13 06:02:01.83]: context ppl 532.7526 | text ppl 48.3695 | rating loss 1.1158 | valid loss 4.9947 on validation
[2022-12-13 06:02:02.37]: epoch 13
[2022-12-13 06:02:59.04]: context ppl 524.0981 | text ppl 42.4827 | rating loss 1.1575 |  1000/ 2583 batches
[2022-12-13 06:03:55.78]: context ppl 525.6679 | text ppl 43.1325 | rating loss 1.1505 |  2000/ 2583 batches
[2022-12-13 06:04:28.82]: context ppl 524.1905 | text ppl 43.1171 | rating loss 1.1503 |  2583/ 2583 batches
[2022-12-13 06:04:34.28]: context ppl 532.3111 | text ppl 48.1972 | rating loss 1.1189 | valid loss 4.9942 on validation
[2022-12-13 06:04:34.83]: epoch 14
[2022-12-13 06:05:31.42]: context ppl 523.5497 | text ppl 42.0121 | rating loss 1.1548 |  1000/ 2583 batches
[2022-12-13 06:06:28.31]: context ppl 522.8348 | text ppl 42.3128 | rating loss 1.1449 |  2000/ 2583 batches
[2022-12-13 06:07:01.45]: context ppl 523.8945 | text ppl 42.4818 | rating loss 1.1601 |  2583/ 2583 batches
[2022-12-13 06:07:06.94]: context ppl 531.0233 | text ppl 47.7667 | rating loss 1.1152 | valid loss 4.9815 on validation
[2022-12-13 06:07:07.68]: epoch 15
[2022-12-13 06:08:04.52]: context ppl 522.1749 | text ppl 41.3229 | rating loss 1.1394 |  1000/ 2583 batches
[2022-12-13 06:09:01.27]: context ppl 521.4006 | text ppl 41.7324 | rating loss 1.1552 |  2000/ 2583 batches
[2022-12-13 06:09:34.33]: context ppl 521.2281 | text ppl 41.8442 | rating loss 1.1553 |  2583/ 2583 batches
[2022-12-13 06:09:39.79]: context ppl 531.1620 | text ppl 47.5283 | rating loss 1.1189 | valid loss 4.9803 on validation
[2022-12-13 06:09:40.33]: epoch 16
[2022-12-13 06:10:37.13]: context ppl 521.1254 | text ppl 40.8482 | rating loss 1.1439 |  1000/ 2583 batches
[2022-12-13 06:11:33.15]: context ppl 520.1638 | text ppl 41.1524 | rating loss 1.1507 |  2000/ 2583 batches
[2022-12-13 06:12:05.70]: context ppl 519.1932 | text ppl 41.2412 | rating loss 1.1495 |  2583/ 2583 batches
[2022-12-13 06:12:11.12]: context ppl 528.9965 | text ppl 47.2900 | rating loss 1.1160 | valid loss 4.9723 on validation
[2022-12-13 06:12:11.66]: epoch 17
[2022-12-13 06:13:07.27]: context ppl 518.3894 | text ppl 40.1949 | rating loss 1.1439 |  1000/ 2583 batches
[2022-12-13 06:14:03.24]: context ppl 518.1798 | text ppl 40.5513 | rating loss 1.1523 |  2000/ 2583 batches
[2022-12-13 06:14:35.87]: context ppl 520.2165 | text ppl 40.9387 | rating loss 1.1412 |  2583/ 2583 batches
[2022-12-13 06:14:41.33]: context ppl 528.4254 | text ppl 47.1817 | rating loss 1.1142 | valid loss 4.9682 on validation
[2022-12-13 06:14:41.86]: epoch 18
[2022-12-13 06:15:38.53]: context ppl 518.9135 | text ppl 39.8193 | rating loss 1.1421 |  1000/ 2583 batches
[2022-12-13 06:16:34.97]: context ppl 516.2804 | text ppl 40.0063 | rating loss 1.1424 |  2000/ 2583 batches
[2022-12-13 06:17:07.80]: context ppl 517.0121 | text ppl 40.1694 | rating loss 1.1502 |  2583/ 2583 batches
[2022-12-13 06:17:13.27]: context ppl 527.6611 | text ppl 46.8912 | rating loss 1.1083 | valid loss 4.9562 on validation
[2022-12-13 06:17:13.81]: epoch 19
[2022-12-13 06:18:09.65]: context ppl 514.3207 | text ppl 38.9563 | rating loss 1.1474 |  1000/ 2583 batches
[2022-12-13 06:19:06.16]: context ppl 516.8810 | text ppl 39.6819 | rating loss 1.1352 |  2000/ 2583 batches
[2022-12-13 06:19:39.06]: context ppl 516.6976 | text ppl 40.0142 | rating loss 1.1539 |  2583/ 2583 batches
[2022-12-13 06:19:44.53]: context ppl 527.5703 | text ppl 46.7332 | rating loss 1.1067 | valid loss 4.9511 on validation
[2022-12-13 06:19:45.04]: epoch 20
[2022-12-13 06:20:41.69]: context ppl 514.4014 | text ppl 38.6205 | rating loss 1.1435 |  1000/ 2583 batches
[2022-12-13 06:21:37.88]: context ppl 514.8917 | text ppl 39.2081 | rating loss 1.1438 |  2000/ 2583 batches
[2022-12-13 06:22:10.52]: context ppl 514.2771 | text ppl 39.4162 | rating loss 1.1416 |  2583/ 2583 batches
[2022-12-13 06:22:15.96]: context ppl 527.2193 | text ppl 46.6460 | rating loss 1.1135 | valid loss 4.9560 on validation
[2022-12-13 06:22:15.96]: Endured 2 time(s)
[2022-12-13 06:22:15.96]: Learning rate set to 0.06250000
[2022-12-13 06:22:15.96]: epoch 21
[2022-12-13 06:23:12.74]: context ppl 509.0814 | text ppl 37.2752 | rating loss 1.1337 |  1000/ 2583 batches
[2022-12-13 06:24:09.10]: context ppl 511.0256 | text ppl 37.4544 | rating loss 1.1386 |  2000/ 2583 batches
[2022-12-13 06:24:42.07]: context ppl 511.5786 | text ppl 37.5771 | rating loss 1.1422 |  2583/ 2583 batches
[2022-12-13 06:24:47.54]: context ppl 524.9268 | text ppl 46.0648 | rating loss 1.1071 | valid loss 4.9372 on validation
[2022-12-13 06:24:48.04]: epoch 22
[2022-12-13 06:25:44.39]: context ppl 510.4248 | text ppl 37.0300 | rating loss 1.1365 |  1000/ 2583 batches
[2022-12-13 06:26:40.51]: context ppl 509.8004 | text ppl 37.0991 | rating loss 1.1343 |  2000/ 2583 batches
[2022-12-13 06:27:13.44]: context ppl 508.7945 | text ppl 37.1447 | rating loss 1.1378 |  2583/ 2583 batches
[2022-12-13 06:27:18.88]: context ppl 524.7495 | text ppl 46.0105 | rating loss 1.1080 | valid loss 4.9369 on validation
[2022-12-13 06:27:19.39]: epoch 23
[2022-12-13 06:28:15.75]: context ppl 509.1111 | text ppl 36.7866 | rating loss 1.1318 |  1000/ 2583 batches
[2022-12-13 06:29:12.20]: context ppl 508.3565 | text ppl 36.9697 | rating loss 1.1390 |  2000/ 2583 batches
[2022-12-13 06:29:44.90]: context ppl 511.4459 | text ppl 36.9793 | rating loss 1.1318 |  2583/ 2583 batches
[2022-12-13 06:29:50.37]: context ppl 523.8170 | text ppl 45.8762 | rating loss 1.1062 | valid loss 4.9322 on validation
[2022-12-13 06:29:50.85]: epoch 24
[2022-12-13 06:30:46.88]: context ppl 509.3310 | text ppl 36.6019 | rating loss 1.1350 |  1000/ 2583 batches
[2022-12-13 06:31:42.65]: context ppl 509.0445 | text ppl 36.9156 | rating loss 1.1350 |  2000/ 2583 batches
[2022-12-13 06:32:15.02]: context ppl 507.8429 | text ppl 36.6151 | rating loss 1.1351 |  2583/ 2583 batches
[2022-12-13 06:32:20.45]: context ppl 524.7281 | text ppl 45.9883 | rating loss 1.1084 | valid loss 4.9368 on validation
[2022-12-13 06:32:20.45]: Endured 3 time(s)
[2022-12-13 06:32:20.45]: Learning rate set to 0.01562500
[2022-12-13 06:32:20.45]: epoch 25
[2022-12-13 06:33:17.17]: context ppl 507.5333 | text ppl 36.2632 | rating loss 1.1291 |  1000/ 2583 batches
[2022-12-13 06:34:13.53]: context ppl 507.9441 | text ppl 36.3695 | rating loss 1.1339 |  2000/ 2583 batches
[2022-12-13 06:34:46.52]: context ppl 507.6542 | text ppl 36.1939 | rating loss 1.1412 |  2583/ 2583 batches
[2022-12-13 06:34:51.97]: context ppl 523.9473 | text ppl 45.7924 | rating loss 1.1063 | valid loss 4.9304 on validation
[2022-12-13 06:34:52.47]: epoch 26
[2022-12-13 06:35:48.90]: context ppl 505.8684 | text ppl 36.0492 | rating loss 1.1386 |  1000/ 2583 batches
[2022-12-13 06:36:45.42]: context ppl 509.1708 | text ppl 36.3698 | rating loss 1.1274 |  2000/ 2583 batches
[2022-12-13 06:37:18.32]: context ppl 507.1002 | text ppl 36.2245 | rating loss 1.1318 |  2583/ 2583 batches
[2022-12-13 06:37:23.76]: context ppl 523.8738 | text ppl 45.7659 | rating loss 1.1069 | valid loss 4.9304 on validation
[2022-12-13 06:37:23.76]: Endured 4 time(s)
[2022-12-13 06:37:23.76]: Learning rate set to 0.00390625
[2022-12-13 06:37:23.76]: epoch 27
[2022-12-13 06:38:20.60]: context ppl 507.3428 | text ppl 36.0674 | rating loss 1.1268 |  1000/ 2583 batches
[2022-12-13 06:39:16.95]: context ppl 508.8598 | text ppl 36.2526 | rating loss 1.1383 |  2000/ 2583 batches
[2022-12-13 06:39:49.93]: context ppl 503.6558 | text ppl 35.8567 | rating loss 1.1303 |  2583/ 2583 batches
[2022-12-13 06:39:55.37]: context ppl 524.1001 | text ppl 45.7831 | rating loss 1.1066 | valid loss 4.9305 on validation
[2022-12-13 06:39:55.37]: Endured 5 time(s)
[2022-12-13 06:39:55.37]: Cannot endure it anymore | Exiting from early stop
=========================================================================================
[2022-12-13 06:40:00.90]: context ppl 528.6172 | text ppl 46.1178 | rating loss 1.1145 on test | End of training
[2022-12-13 06:40:00.90]: Generating text
[2022-12-13 06:40:49.56]: RMSE  1.0557
[2022-12-13 06:40:49.96]: MAE  0.8296
[2022-12-13 06:40:52.45]: BLEU-1 18.5701
[2022-12-13 06:40:58.09]: BLEU-4  2.6422
[2022-12-13 06:41:15.34]: USR  0.2531 | USN    9323
[2022-12-13 06:45:25.67]: DIV  1.2767
[2022-12-13 06:45:26.88]: FCR  0.2815
[2022-12-13 06:45:26.89]: FMR  0.7914
[2022-12-13 06:45:32.51]: rouge_1/f_score 25.8881
[2022-12-13 06:45:32.51]: rouge_1/r_score 23.1487
[2022-12-13 06:45:32.51]: rouge_1/p_score 35.0430
[2022-12-13 06:45:32.51]: rouge_2/f_score  6.4174
[2022-12-13 06:45:32.51]: rouge_2/r_score  5.9180
[2022-12-13 06:45:32.51]: rouge_2/p_score  8.8020
[2022-12-13 06:45:32.51]: rouge_l/f_score 19.3052
[2022-12-13 06:45:32.51]: rouge_l/r_score 20.5859
[2022-12-13 06:45:32.51]: rouge_l/p_score 28.4370
[2022-12-13 06:45:32.59]: Generated text saved to (FMLPETER/output/generated.txt)
